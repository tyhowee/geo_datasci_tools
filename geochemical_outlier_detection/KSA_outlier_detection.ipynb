{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <u>Multivariate Outlier Detection in Geochemical Datasets</u>\n",
    "\n",
    "This notebook is intended as an open-source resource for exploring, analyzing and comparing three different methods of outlier detection in geochemical datasets in the context of mineral exploration.  \n",
    "<br />\n",
    "<br />\n",
    "The three primary outlier detection algorithms we will use are the following: \n",
    "- Isolation Forests (IF) (Liu et al., 2008)\n",
    "- Local Outlier Factor (LOF) (Breunig et al., 2000)\n",
    "- Angle Based Outlier Detection (ABOD) (Shahrestani & Sanislav, 2025)\n",
    "<br />\n",
    "<br />\n",
    "\n",
    "This work is driven from the findings in Antoine Cat√©'s article on multivariate outlier detection for mineral exploration.\n",
    "\n",
    "<br />\n",
    "<br />\n",
    "<u>References: </u>\n",
    "\n",
    "*Breunig, M. M., Kriegel, H.-P., Ng, R. T., and Sander, J., 2000, LOF: Identifying Density-Based Local Outliers: ACM SIGMOD Record, v. 29, no. 2, p. 93-104.*\n",
    "\n",
    "*Liu, F. T., Ting, K. M., and Zhou, Z.-H., 2008, Isolation Forest, 2008 Eighth IEEE International Conference on Data Mining, p. 413-422.*\n",
    "\n",
    "*Maklin, C., 2022, Isolation Forest - Cory Maklin - Medium: Medium, https://medium.com/@corymaklin/isolation-forest-799fceacdda4.*\n",
    "\n",
    "*Shahrestani, S., and Sanislav, I., 2025, Mapping geochemical anomalies using angle-based outlier detection approach: Journal of Geochemical Exploration, v. 269.*\n",
    "<br />\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import helper_functions\n",
    "import outlier_detection_functions\n",
    "\n",
    "import importlib\n",
    "importlib.reload(helper_functions)\n",
    "importlib.reload(outlier_detection_functions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*For the testing of these algorithms we are going to use geochemical data from a region in southwestern Saudi Arabia. All units are converted to ppm for consistency.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet('data_files/KSA_raw_data.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of all geochemical columns to be used in the analysis - omits sample number and location columns\n",
    "# List of all geochemical columns to be used in the analysis - omits sample number and location columns\n",
    "feature_columns = [\n",
    "    col for col in df.columns if col not in [\"Sample Field Number\", \"Longitude\", \"Latitude\"]\n",
    "]\n",
    "\n",
    "print(f\"Selected feature columns: {feature_columns}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <u>Simple EDA on the data</u>\n",
    "\n",
    "To start off, let's do a brief investigation into broad trends or relationships in the data to get a sense of what we are working with. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate pairplot of select elements\n",
    "elements_to_plot = [\"TFe2O3\", \"Co\", \"V\", \"Rb\"]  # Choose key elements - ideally those related to mineralization or other interesting trends\n",
    "helper_functions.generate_pairplot(df, elements_to_plot, height=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate correlation heatmap\n",
    "helper_functions.plot_correlation_heatmap(df, feature_columns, figsize=(10,8), annot=False) # Increase figsize to view all element labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA\n",
    "pc1_scores, top_features1, top_features2 = helper_functions.generate_pca(df, feature_columns)\n",
    "print(\"Top 5 Contributing Features to PC1:\", top_features1)\n",
    "print(\"Top 5 Contributing Features to PC2:\", top_features2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## <u>Outlier Detection Methods</u>\n",
    "\n",
    "Outliers in the binary plots below are classified using the Modified Z-Score method based on the Median Absolute Deviation (MAD). Data points with a modified Z-score greater than 3.5 are labeled as outliers (-1). This is done in place of the standard Contamination value, as it is hard to manually estimate the proportion of outliers in the datset.\n",
    "\n",
    "### <u>Isolation Forest</u>\n",
    "\n",
    "Isolation forest is an unsupervised machine learning method of outlier/anomaly detection. It is an ensemble method that combines the predictions of several decision trees to assign an anomaly score to a given data point. Samples that require fewer splits across all trees are given a lower anomaly score (higher likelihood of being anomalous). This method of outlier detection is not affected by data distribution, but does require some parameter-tuning. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IF_df = outlier_detection_functions.isolation_forest(\n",
    "    df, feature_columns\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting IF results\n",
    "helper_functions.plot_outlier_results(\n",
    "    data=IF_df,\n",
    "    x_col=\"Longitude\",\n",
    "    y_col=\"Latitude\",\n",
    "    score_col=\"anomaly_score\",\n",
    "    binary_col=\"outlier\",\n",
    "    point_size=50,\n",
    "    plot_title=\"IF Outlier Detection Results\",\n",
    "    cmap='viridis_r'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### <u>Local Outlier Factor</u>\n",
    "\n",
    "LOF is another unsupervised outlier detection method that uses a density-based approach, comparing the density of data points in their local neighborhoods. Isolated samples or those on the margins of a neighborhood cluster will have a lower density than their neighbors. Samples with a lower LOF value are considered outliers. Similar to IF, LOF is unaffected by data distribution but does require some parameter-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOF_df = outlier_detection_functions.local_outlier_factor(\n",
    "    df, feature_columns, n_neighbors=50, scale_data=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting LOF results\n",
    "helper_functions.plot_outlier_results(\n",
    "    data=LOF_df,\n",
    "    x_col=\"Longitude\",\n",
    "    y_col=\"Latitude\",\n",
    "    score_col=\"anomaly_score\",\n",
    "    binary_col=\"outlier\",\n",
    "    point_size=50,\n",
    "    plot_title=\"LOF Outlier Detection Results\",\n",
    "    cmap=\"viridis_r\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### <u>Angle Based Outlier Detection</u>\n",
    "\n",
    "Our final method of outlier detection is angle based outlier detection. This method compares the distribution of angles of distance vectors between a sample point and its neighbors; a point with a large distribution of vector angles is considered an inlier (within a cluster), while a point with a smaller distribution of angles is likely an outlier (outside a cluster). An angle-based score is then calculated, with less variation indicating a higher probability of the sample point being an outlier. One benefit of ABOD is that it is free of any parameters, and thus does not have the potential prediction variability resulting from tuning. Its implementation does not, however, output a binary classification, thus in order to generate one an arbitrary threshold must be defined. \n",
    "\n",
    "***Note: running the full ABOD function, with use_knn=False, takes quite a long time to process, especially for larger datasets (n>1000). It is recommended to downsample using knn if running on a large dataset, but know that the model accuracy may be impacted.*** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ABOD_df = outlier_detection_functions.abod(df, feature_columns, scale_data=True, use_knn=False, k_neighbors=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting ABOD results\n",
    "helper_functions.plot_outlier_results(\n",
    "    data=ABOD_df,\n",
    "    x_col=\"Longitude\",\n",
    "    y_col=\"Latitude\",\n",
    "    score_col=\"anomaly_score\",\n",
    "    binary_col=\"outlier\",\n",
    "    point_size=50,\n",
    "    plot_title=\"ABOD Outlier Detection Results\",\n",
    "    cmap=\"viridis_r\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## <u>Validation of predictions</u>\n",
    "\n",
    "Multivariate anomaly detection picks up on trends across multiple elements, rather than just single-element variation. As opposed to univariate anomalies that may be attributed to noise, sampling error, or highly-localized trends, multivariate analysis may point towards regions of broader geologic alteration related to mineral deposits. To test this relationship, we will compare outlier predictions from each model against known mineral occurrences in the sampling region. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_df = pd.read_parquet('data_files/KSA_validation.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Spatial validation of outlier predictions*\n",
    "\n",
    "To start, we will conduct a visual analysis on the data, comparing binary outlier classifications vs. known mineral occurrences in the region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot outlier detection results and validation dataset\n",
    "\n",
    "outlier_results = [IF_df, LOF_df, ABOD_df]\n",
    "outlier_result_names = ['IF', 'LOF', 'ABOD']\n",
    "\n",
    "helper_functions.plot_validation(outlier_datasets=outlier_results, outlier_dataset_names=outlier_result_names, validation_df=validation_df, point_size=10, colormap='viridis')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### *ROC-AUC, ANOVA F-statistic, and Mutual Information scoring of outlier predictions*\n",
    "\n",
    "To quantitatively compare the different outlier prediction methods, we will use three different scoring methods: ROC-AUC, ANOVA F-statistic, and Mutual Information.\n",
    "<br />\n",
    "<br />\n",
    "- *<u>ROC-AUC, or Receiver Operating Characteristic Area Under the Curve</u>* is a machine learning metric used to evaluate a model's ability to distinguish between positive and negative classes; a score of 1 is considered 'perfect,' or that the model gets 100% of predictions correct. \n",
    "    - Generally best for evaluating overall predictive performance, regardless of spatial location.\n",
    "<br />\n",
    "<br />\n",
    "- The *<u>ANOVA F-statistic</u>* is a method of comparing the variances of two samples (in this case the prediction and the validation set); the higher the score, the greater the model's predictions are differentiated. That is, there is a meaningful pattern between predicted outliers and known mineral deposits.\n",
    "    - Measures how well the model distinguishes spatially relevant anomalies.\n",
    "<br />\n",
    "<br />\n",
    "- *<u>Mutual Information</u>* is a method of measuring how much information one variable provides about another, or how dependent they are on each other. Higher MI values indicate stronger relationships between variables. \n",
    "    - Quantifies dependency between outlier predictions and proximity to known deposits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate scores for each outlier detection method\n",
    "\n",
    "scoring_radius = 0.005 # roughly 500m\n",
    "\n",
    "roc_auc_scores = helper_functions.calculate_roc_auc(\n",
    "    outlier_results, outlier_result_names, validation_df, radius=scoring_radius\n",
    ") \n",
    "\n",
    "f_scores = helper_functions.calculate_f_score(\n",
    "    outlier_results, outlier_result_names, validation_df, radius=scoring_radius\n",
    ")\n",
    "\n",
    "mi_scores = helper_functions.calculate_mi_score(\n",
    "    outlier_results, outlier_result_names, validation_df, radius=scoring_radius\n",
    ")\n",
    "\n",
    "helper_functions.plot_scores(\n",
    "    [roc_auc_scores, f_scores, mi_scores],\n",
    "    titles=[\"ROC-AUC Scores\", \"F-Scores\", \"Mutual Information Scores\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### *Deeper investigation into ROC-AUC scoring of each model*\n",
    "\n",
    "Since the ROC-AUC scores for each model above are so similar, let's create a ROC curve to better understand the model performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "helper_functions.plot_roc_curves(\n",
    "    outlier_datasets=outlier_results,  # Your outlier model DataFrames\n",
    "    outlier_dataset_names=outlier_result_names,  # Names of models\n",
    "    validation_df=validation_df,  # Known mineral deposits\n",
    "    radius=scoring_radius,  # Search radius\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ROC curve above matches the results we observed - the ABOD method appears to best optimize the TPR/FPR ratio, indicating better model performance. \n",
    "\n",
    "---\n",
    "\n",
    "### <u>Time Cost Analysis</u>\n",
    "\n",
    "Despite the quantitative accuracies between models, it is also important to consider the time cost of each; IF and LOF process almost instantly, while ABOD takes some time due to the nature of the calculation. This ABOD function is set up to allow for using k-nearest neighbors to calculate variance rather than using all possible pairs in the dataset, which improves processing time while potentially skewing results. For relatively small datasets, the time cost of ABOD is minimal, but for larger datasets the tradeoffs should be considered; it may be more efficient to use an algorithm like Isolation Forest despite the small cost in model accuracy. Further investigation into the effects of kNN on model accuracy is needed. \n",
    "\n",
    "Below we will do a brief investigation into the time cost of each method, focusing particularly on ABOD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore time cost of outlier detection vs. roc-auc score\n",
    "\n",
    "outlier_models = [\n",
    "    outlier_detection_functions.isolation_forest, \n",
    "    outlier_detection_functions.local_outlier_factor, \n",
    "    outlier_detection_functions.abod\n",
    "]\n",
    "\n",
    "outlier_model_names = ['IF', 'LOF', 'ABOD']\n",
    "scoring_radius = 0.005\n",
    "\n",
    "# Dictionary to store results\n",
    "results = {\n",
    "    \"Model\": [],\n",
    "    \"Iteration\": [],\n",
    "    \"Execution Time (s)\": [],\n",
    "    \"ROC-AUC Score\": []\n",
    "}\n",
    "\n",
    "for model, name in zip(outlier_models, outlier_model_names):\n",
    "    for i in range(7):\n",
    "        start = time.time()\n",
    "        output_df = model(df, feature_columns)\n",
    "        end = time.time()\n",
    "        exec_time = end - start\n",
    "\n",
    "        print(f\"{model.__name__} iteration {i} took {end - start} seconds\")\n",
    "\n",
    "        roc_auc_score = helper_functions.calculate_roc_auc(\n",
    "            outlier_datasets=[output_df],\n",
    "            outlier_dataset_names=[name],\n",
    "            validation_df=validation_df,\n",
    "            radius=scoring_radius,\n",
    "        )[name]\n",
    "\n",
    "        # Store results in dictionary\n",
    "        results[\"Model\"].append(name)\n",
    "        results[\"Iteration\"].append(i)\n",
    "        results[\"Execution Time (s)\"].append(exec_time)\n",
    "        results[\"ROC-AUC Score\"].append(roc_auc_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the results of the above analysis\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
    "\n",
    "# Box Plot\n",
    "sns.boxplot(x=\"Model\", y=\"Execution Time (s)\", data=results_df, ax=axes[0])\n",
    "axes[0].set_title(\"Execution Times by Model\")\n",
    "axes[0].set_xlabel(\"Outlier Detection Model\")\n",
    "axes[0].set_ylabel(\"Execution Time (s)\")\n",
    "axes[0].set_yscale(\"log\")\n",
    "\n",
    "# Scatter Plot\n",
    "sns.scatterplot(\n",
    "    x=\"Execution Time (s)\", y=\"ROC-AUC Score\", hue=\"Model\", data=results_df, ax=axes[1]\n",
    ")\n",
    "axes[1].set_title(\"Execution Time vs. ROC-AUC Score\")\n",
    "axes[1].set_xlabel(\"Execution Time (s)\")\n",
    "axes[1].set_xscale(\"log\")\n",
    "axes[1].set_ylabel(\"ROC-AUC Score\")\n",
    "axes[1].legend(title=\"Model\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see in the above plots, IF and LOF have significantly lower execution times. IF seems to strike somewhat of a balance between execution time and ROC-AUC score, indicating it may be more suitable than ABOD for datasets n > ~2000-3000 as the tradeoff between time and score diminishes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the execution time of each method with varying sample sizes\n",
    "\n",
    "# Define sample sizes\n",
    "sample_sizes = [50, 100, 200, 400, 800, 1600, 2700]\n",
    "\n",
    "# Measure execution time\n",
    "abod_results_df = helper_functions.measure_model_execution(\n",
    "    df, sample_sizes, model=outlier_detection_functions.abod\n",
    ")\n",
    "\n",
    "if_results_df = helper_functions.measure_model_execution(\n",
    "    df, sample_sizes, model=outlier_detection_functions.isolation_forest\n",
    ")\n",
    "\n",
    "lof_results_df = helper_functions.measure_model_execution(\n",
    "    df, sample_sizes, model=outlier_detection_functions.local_outlier_factor\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the above results\n",
    "fig, axes = plt.subplots(1, 3, figsize=(14, 4))\n",
    "\n",
    "# ABOD\n",
    "axes[0].plot(\n",
    "    abod_results_df[\"Number of Samples\"],\n",
    "    abod_results_df[\"Execution Time (s)\"],\n",
    "    marker=\"o\",\n",
    "    linestyle=\"-\",\n",
    ")\n",
    "axes[0].set_xscale(\"log\") \n",
    "axes[0].set_yscale(\"log\")\n",
    "axes[0].set_xlabel(\"Number of Samples (Log Scale)\")\n",
    "axes[0].set_ylabel(\"Execution Time (s) (Log Scale)\")\n",
    "axes[0].set_title(\"ABOD Method\")\n",
    "axes[0].grid(True, which=\"both\", linestyle=\"--\", linewidth=0.5)\n",
    "\n",
    "# IF\n",
    "axes[1].plot(\n",
    "    if_results_df[\"Number of Samples\"],\n",
    "    if_results_df[\"Execution Time (s)\"],\n",
    "    marker=\"o\",\n",
    "    linestyle=\"-\",\n",
    ")\n",
    "axes[1].set_xlabel(\"Number of Samples (Log Scale)\")\n",
    "axes[1].set_xscale(\"log\")  \n",
    "axes[1].set_ylabel(\"Execution Time (s)\")\n",
    "axes[1].set_ylim(0, 0.07)\n",
    "axes[1].set_title(\"IF Method\")\n",
    "axes[1].grid(True, which=\"both\", linestyle=\"--\", linewidth=0.5)\n",
    "\n",
    "# LOF\n",
    "axes[2].plot(\n",
    "    lof_results_df[\"Number of Samples\"],\n",
    "    lof_results_df[\"Execution Time (s)\"],\n",
    "    marker=\"o\",\n",
    "    linestyle=\"-\",\n",
    ")\n",
    "axes[2].set_xlabel(\"Number of Samples (Log Scale)\")\n",
    "axes[2].set_xscale(\"log\")  \n",
    "axes[2].set_ylabel(\"Execution Time (s)\")\n",
    "axes[2].set_ylim(0, 0.07)\n",
    "axes[2].set_title(\"LOF Method\")\n",
    "axes[2].grid(True, which=\"both\", linestyle=\"--\", linewidth=0.5)\n",
    "\n",
    "fig.suptitle(\"Number of Samples vs. Execution Time per Outlier Detection Method\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see above, ABOD has more similar execution times as IF/LOF (<1s) when n<~200, but beyond that the time cost grows exponentially. IF and LOF distributions are irregular due to the extremely short processing times, and variation can be attributed to random noise; they generally follow an O(n) time complexity, while ABOD appears to follow O(n^2), which aligns with the pairwise computations required to calculate ABOD. \n",
    "\n",
    "To summarize, ABOD seems to be the most appropriate model for smaller datasets (n<2000-3000), but beyond that the time cost becomes significant, and IF should be considered as a still plenty-capable alternative. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
