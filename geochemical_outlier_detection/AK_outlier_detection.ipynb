{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <u>Multivariate Outlier Detection in Geochemical Datasets</u>\n",
    "\n",
    "This notebook is intended as an open-source resource for exploring, analyzing and comparing three different methods of outlier detection in geochemical datasets in the context of mineral exploration.  \n",
    "<br />\n",
    "<br />\n",
    "The three primary outlier detection algorithms we will use are the following: \n",
    "- Isolation Forests (IF) (Liu et al., 2008)\n",
    "- Local Outlier Factor (LOF) (Breunig et al., 2000)\n",
    "- Angle Based Outlier Detection (ABOD) (Shahrestani & Sanislav, 2025)\n",
    "<br />\n",
    "<br />\n",
    "\n",
    "This work is driven from the findings in Antoine Cat√©'s article on multivariate outlier detection for mineral exploration.\n",
    "\n",
    "<br />\n",
    "<br />\n",
    "<u>References: </u>\n",
    "\n",
    "*Breunig, M. M., Kriegel, H.-P., Ng, R. T., and Sander, J., 2000, LOF: Identifying Density-Based Local Outliers: ACM SIGMOD Record, v. 29, no. 2, p. 93-104.*\n",
    "\n",
    "*Liu, F. T., Ting, K. M., and Zhou, Z.-H., 2008, Isolation Forest, 2008 Eighth IEEE International Conference on Data Mining, p. 413-422.*\n",
    "\n",
    "*Maklin, C., 2022, Isolation Forest - Cory Maklin - Medium: Medium, https://medium.com/@corymaklin/isolation-forest-799fceacdda4.*\n",
    "\n",
    "*Shahrestani, S., and Sanislav, I., 2025, Mapping geochemical anomalies using angle-based outlier detection approach: Journal of Geochemical Exploration, v. 269.*\n",
    "<br />\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import helper_functions\n",
    "import outlier_detection_functions\n",
    "\n",
    "import importlib\n",
    "importlib.reload(helper_functions)\n",
    "importlib.reload(outlier_detection_functions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*For the testing of these algorithms we are going to use geochemical data from a region in southeastern Alaska, USA. All units are converted to ppm for consistency.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(\"data_files/AK_raw_data.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot percentage of missing data in each column\n",
    "helper_functions.plot_nan_percentage(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove columns with more than 60% missing data and where the mode occurs in more than 40% of rows, fill missing data with median\n",
    "df = helper_functions.clean_geochemical_data(df, nan_threshold=0.6, mode_threshold=0.4)\n",
    "print(f\"Columns in cleaned dataset: {df.columns}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of all geochemical columns to be used in the analysis - omits sample number and location columns\n",
    "feature_columns = [\n",
    "    col for col in df.columns if col not in [\"lab_id\", \"latitude\", \"longitude\"]\n",
    "]\n",
    "\n",
    "print(f\"Selected feature columns: {feature_columns}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <u>Simple EDA on the data</u>\n",
    "\n",
    "To start off, let's do a brief investigation into broad trends or relationships in the data to get a sense of what we are working with. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate pairplot of select elements\n",
    "elements_to_plot = [\"Co_ppm\", \"Cu_ppm\", \"Ni_ppm\", \"Pb_ppm\"]  # Choose key elements - ideally those related to mineralization or other interesting trends\n",
    "helper_functions.generate_pairplot(df, elements_to_plot, height=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute mode (first mode value) and median for each feature\n",
    "\n",
    "helper_functions.plot_mode_median(df, feature_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count number of modes per column\n",
    "\n",
    "helper_functions.plot_mode_percentage(df, feature_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate correlation heatmap\n",
    "helper_functions.plot_correlation_heatmap(df, feature_columns, figsize=(10,8), annot=False) # Increase figsize to view all element labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA\n",
    "pc1_scores, top_features1, top_features2 = helper_functions.generate_pca(df, feature_columns)\n",
    "print(\"Top 5 Contributing Features to PC1:\", top_features1)\n",
    "print(\"Top 5 Contributing Features to PC2:\", top_features2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## <u>Outlier Detection Methods</u>\n",
    "\n",
    "Outliers in the binary plots below are classified using the Modified Z-Score method based on the Median Absolute Deviation (MAD). Data points with a modified Z-score greater than 3.5 are labeled as outliers (-1). This is done in place of the standard Contamination value, as it is hard to manually estimate the proportion of outliers in the datset.\n",
    "\n",
    "### <u>Isolation Forest</u>\n",
    "\n",
    "Isolation forest is an unsupervised machine learning method of outlier/anomaly detection. It is an ensemble method that combines the predictions of several decision trees to assign an anomaly score to a given data point. Samples that require fewer splits across all trees are given a lower anomaly score (higher likelihood of being anomalous). This method of outlier detection is not affected by data distribution, but does require some parameter-tuning. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IF_df = outlier_detection_functions.isolation_forest(\n",
    "    df, feature_columns\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting IF results\n",
    "helper_functions.plot_outlier_results(\n",
    "    data=IF_df,\n",
    "    x_col=\"longitude\",\n",
    "    y_col=\"latitude\",\n",
    "    score_col=\"anomaly_score\",\n",
    "    binary_col=\"outlier\",\n",
    "    point_size=50,\n",
    "    plot_title=\"IF Outlier Detection Results\",\n",
    "    cmap='viridis_r'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### <u>Local Outlier Factor</u>\n",
    "\n",
    "LOF is another unsupervised outlier detection method that uses a density-based approach, comparing the density of data points in their local neighborhoods. Isolated samples or those on the margins of a neighborhood cluster will have a lower density than their neighbors. Samples with a lower LOF value are considered outliers. Similar to IF, LOF is unaffected by data distribution but does require some parameter-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOF_df = outlier_detection_functions.local_outlier_factor(\n",
    "    df, feature_columns, n_neighbors=50, scale_data=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting LOF results\n",
    "helper_functions.plot_outlier_results(\n",
    "    data=LOF_df,\n",
    "    x_col=\"longitude\",\n",
    "    y_col=\"latitude\",\n",
    "    score_col=\"anomaly_score\",\n",
    "    binary_col=\"outlier\",\n",
    "    point_size=50,\n",
    "    plot_title=\"LOF Outlier Detection Results\",\n",
    "    cmap=\"viridis_r\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### <u>Angle Based Outlier Detection</u>\n",
    "\n",
    "Our final method of outlier detection is angle based outlier detection. This method compares the distribution of angles of distance vectors between a sample point and its neighbors; a point with a large distribution of vector angles is considered an inlier (within a cluster), while a point with a smaller distribution of angles is likely an outlier (outside a cluster). An angle-based score is then calculated, with less variation indicating a higher probability of the sample point being an outlier. One benefit of ABOD is that it is free of any parameters, and thus does not have the potential prediction variability resulting from tuning. Its implementation does not, however, output a binary classification, thus in order to generate one an arbitrary threshold must be defined. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ABOD_df = outlier_detection_functions.abod(df, feature_columns, scale_data=True, use_knn=True, k_neighbors=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting ABOD results\n",
    "helper_functions.plot_outlier_results(\n",
    "    data=ABOD_df,\n",
    "    x_col=\"longitude\",\n",
    "    y_col=\"latitude\",\n",
    "    score_col=\"anomaly_score\",\n",
    "    binary_col=\"outlier\",\n",
    "    point_size=50,\n",
    "    plot_title=\"ABOD Outlier Detection Results\",\n",
    "    cmap=\"viridis_r\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## <u>Validation of predictions</u>\n",
    "\n",
    "Multivariate anomaly detection picks up on trends across multiple elements, rather than just single-element variation. As opposed to univariate anomalies that may be attributed to noise, sampling error, or highly-localized trends, multivariate analysis may point towards regions of broader geologic alteration related to mineral deposits. To test this relationship, we will compare outlier predictions from each model against known mineral occurrences in the sampling region. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_df = pd.read_parquet('data_files/AK_validation.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Spatial validation of outlier predictions*\n",
    "\n",
    "To start, we will conduct a visual analysis on the data, comparing binary outlier classifications vs. known mineral occurrences in the region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot outlier detection results and validation dataset\n",
    "\n",
    "outlier_results = [IF_df, LOF_df, ABOD_df]\n",
    "outlier_result_names = ['IF', 'LOF', 'ABOD']\n",
    "\n",
    "helper_functions.plot_validation(outlier_datasets=outlier_results, outlier_dataset_names=outlier_result_names, validation_df=validation_df, point_size=10, colormap='viridis', x_col='longitude', y_col='latitude')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### *ROC-AUC, ANOVA F-statistic, and Mutual Information scoring of outlier predictions*\n",
    "\n",
    "To quantitatively compare the different outlier prediction methods, we will use three different scoring methods: ROC-AUC, ANOVA F-statistic, and Mutual Information.\n",
    "<br />\n",
    "<br />\n",
    "- *<u>ROC-AUC, or Receiver Operating Characteristic Area Under the Curve</u>* is a machine learning metric used to evaluate a model's ability to distinguish between positive and negative classes; a score of 1 is considered 'perfect,' or that the model gets 100% of predictions correct. \n",
    "    - Generally best for evaluating overall predictive performance, regardless of spatial location.\n",
    "<br />\n",
    "<br />\n",
    "- The *<u>ANOVA F-statistic</u>* is a method of comparing the variances of two samples (in this case the prediction and the validation set); the higher the score, the greater the model's predictions are differentiated. That is, there is a meaningful pattern between predicted outliers and known mineral deposits.\n",
    "    - Measures how well the model distinguishes spatially relevant anomalies.\n",
    "<br />\n",
    "<br />\n",
    "- *<u>Mutual Information</u>* is a method of measuring how much information one variable provides about another, or how dependent they are on each other. Higher MI values indicate stronger relationships between variables. \n",
    "    - Quantifies dependency between outlier predictions and proximity to known deposits.\n",
    "\n",
    "\n",
    "*Note: due to the large sample size of the dataset and the way that F- and MI scores are calculated, the results from these two are skewed.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate scores for each outlier detection method\n",
    "\n",
    "scoring_radius = 0.005 # roughly 500m\n",
    "\n",
    "roc_auc_scores = helper_functions.calculate_roc_auc(\n",
    "    outlier_results,\n",
    "    outlier_result_names,\n",
    "    validation_df,\n",
    "    radius=scoring_radius,\n",
    "    x_col=\"longitude\",\n",
    "    y_col=\"latitude\",\n",
    ")\n",
    "\n",
    "f_scores = helper_functions.calculate_f_score(\n",
    "    outlier_results,\n",
    "    outlier_result_names,\n",
    "    validation_df,\n",
    "    radius=scoring_radius,\n",
    "    x_col=\"longitude\",\n",
    "    y_col=\"latitude\",\n",
    ")\n",
    "\n",
    "mi_scores = helper_functions.calculate_mi_score(\n",
    "    outlier_results,\n",
    "    outlier_result_names,\n",
    "    validation_df,\n",
    "    radius=scoring_radius,\n",
    "    x_col=\"longitude\",\n",
    "    y_col=\"latitude\",\n",
    ")\n",
    "\n",
    "helper_functions.plot_scores(\n",
    "    [roc_auc_scores, f_scores, mi_scores],\n",
    "    titles=[\"ROC-AUC Scores\", \"F-Scores\", \"Mutual Information Scores\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### *Deeper investigation into ROC-AUC scoring of each model*\n",
    "\n",
    "Lets create a ROC curve for each model to better understand the way that they are performing compared to each other. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "helper_functions.plot_roc_curves(\n",
    "    outlier_datasets=outlier_results,  # Your outlier model DataFrames\n",
    "    outlier_dataset_names=outlier_result_names,  # Names of models\n",
    "    validation_df=validation_df,  # Known mineral deposits\n",
    "    radius=scoring_radius,\n",
    "    x_col='longitude', \n",
    "    y_col='latitude'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ROC curve above generally matches the results we observed in the first peek into model performance - IF best optimizes the TPR/FPR ratio, closely followed by LOF. ABOD actually skews towards the false positive side of the graph. This may be due to the fact that we are utilizing k-nearest neighbors to optimize its processing time. Unfortunately, at least on the system this code was created on, to process the full dataset would take approx. 250 hours, which is not feasible for this analysis. Further investigation into the effects of kNN on model accuracy is needed. \n",
    "\n",
    "---\n",
    "\n",
    "### <u>Time Cost Analysis</u>\n",
    "\n",
    "Given the significant time-cost differences between models, it is important to consider how they compare relative to model accuracy; IF and LOF process almost instantly, while ABOD takes some time due to the nature of the calculation. For relatively small datasets, the time cost of ABOD is minimal, but for larger datasets the tradeoffs should be considered; it may be more efficient to use an algorithm like Isolation Forest.\n",
    "\n",
    "Below we will do a brief investigation into the time cost of each method, focusing particularly on ABOD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier_models = [\n",
    "    outlier_detection_functions.isolation_forest,\n",
    "    outlier_detection_functions.local_outlier_factor,\n",
    "    lambda df, feature_columns: outlier_detection_functions.abod(\n",
    "        df, feature_columns, use_knn=True, k_neighbors=50\n",
    "    ),\n",
    "]\n",
    "\n",
    "outlier_model_names = [\"IF\", \"LOF\", \"ABOD\"]\n",
    "scoring_radius = 0.005\n",
    "\n",
    "# Dictionary to store results\n",
    "results = {\"Model\": [], \"Iteration\": [], \"Execution Time (s)\": [], \"ROC-AUC Score\": []}\n",
    "\n",
    "for model, name in zip(outlier_models, outlier_model_names):\n",
    "    for i in range(7):\n",
    "        start = time.time()\n",
    "        output_df = model(df, feature_columns)  # abod will be called with use_knn=True\n",
    "        end = time.time()\n",
    "        exec_time = end - start\n",
    "\n",
    "        print(f\"{name} iteration {i} took {exec_time} seconds\")\n",
    "\n",
    "        roc_auc_score = helper_functions.calculate_roc_auc(\n",
    "            outlier_datasets=[output_df],\n",
    "            outlier_dataset_names=[name],\n",
    "            validation_df=validation_df,\n",
    "            radius=scoring_radius,\n",
    "            x_col=\"longitude\",\n",
    "            y_col=\"latitude\",\n",
    "        )[name]\n",
    "\n",
    "        # Store results in dictionary\n",
    "        results[\"Model\"].append(name)\n",
    "        results[\"Iteration\"].append(i)\n",
    "        results[\"Execution Time (s)\"].append(exec_time)\n",
    "        results[\"ROC-AUC Score\"].append(roc_auc_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the results of the above analysis\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
    "\n",
    "# Box Plot\n",
    "sns.boxplot(x=\"Model\", y=\"Execution Time (s)\", data=results_df, ax=axes[0])\n",
    "axes[0].set_title(\"Execution Times by Model\")\n",
    "axes[0].set_xlabel(\"Outlier Detection Model\")\n",
    "axes[0].set_ylabel(\"Execution Time (s)\")\n",
    "axes[0].set_yscale(\"log\")\n",
    "\n",
    "# Scatter Plot\n",
    "sns.scatterplot(\n",
    "    x=\"Execution Time (s)\", y=\"ROC-AUC Score\", hue=\"Model\", data=results_df, ax=axes[1]\n",
    ")\n",
    "axes[1].set_title(\"Execution Time vs. ROC-AUC Score\")\n",
    "axes[1].set_xlabel(\"Execution Time (s)\")\n",
    "axes[1].set_xscale(\"log\")\n",
    "axes[1].set_ylabel(\"ROC-AUC Score\")\n",
    "axes[1].legend(title=\"Model\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see in the above plots, IF has both a lower execution time as well as higher ROC-AUC score than LOF, which is then significantly faster and more accurate than ABOD. Given this, it does not seem practical to utilize LOF or ABOD, at least in the context of the given dataset. It is important to note that the ABOD function run in this analysis utilizes kNN downsampling to speed up processing times, which may have an impact on model accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the execution time of each method with varying sample sizes\n",
    "\n",
    "# Define sample sizes\n",
    "sample_sizes = [50, 100, 200, 400, 800]\n",
    "\n",
    "# Measure execution time\n",
    "abod_results_df = helper_functions.measure_model_execution(\n",
    "    df, sample_sizes, model=outlier_detection_functions.abod\n",
    ")\n",
    "\n",
    "if_results_df = helper_functions.measure_model_execution(\n",
    "    df, sample_sizes, model=outlier_detection_functions.isolation_forest\n",
    ")\n",
    "\n",
    "lof_results_df = helper_functions.measure_model_execution(\n",
    "    df, sample_sizes, model=outlier_detection_functions.local_outlier_factor\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the results\n",
    "fig, axes = plt.subplots(1, 3, figsize=(14, 4))\n",
    "\n",
    "# IF\n",
    "axes[0].plot(\n",
    "    if_results_df[\"Number of Samples\"],\n",
    "    if_results_df[\"Execution Time (s)\"],\n",
    "    marker=\"o\",\n",
    "    linestyle=\"-\",\n",
    ")\n",
    "axes[0].set_xlabel(\"Number of Samples (Log Scale)\")\n",
    "axes[0].set_xscale(\"log\")  \n",
    "axes[0].set_ylabel(\"Execution Time (s)\")\n",
    "axes[0].set_ylim(0.08, 0.12)\n",
    "axes[0].set_title(\"IF Method\")\n",
    "axes[0].grid(True, which=\"both\", linestyle=\"--\", linewidth=0.5)\n",
    "\n",
    "# LOF\n",
    "axes[1].plot(\n",
    "    lof_results_df[\"Number of Samples\"],\n",
    "    lof_results_df[\"Execution Time (s)\"],\n",
    "    marker=\"o\",\n",
    "    linestyle=\"-\",\n",
    ")\n",
    "axes[1].set_xlabel(\"Number of Samples (Log Scale)\")\n",
    "axes[1].set_xscale(\"log\")  \n",
    "axes[1].set_ylabel(\"Execution Time (s)\")\n",
    "axes[1].set_ylim(0, 0.08)\n",
    "axes[1].set_title(\"LOF Method\")\n",
    "axes[1].grid(True, which=\"both\", linestyle=\"--\", linewidth=0.5)\n",
    "\n",
    "## ABOD\n",
    "axes[2].plot(\n",
    "   abod_results_df[\"Number of Samples\"],\n",
    "   abod_results_df[\"Execution Time (s)\"],\n",
    "   marker=\"o\",\n",
    "   linestyle=\"-\",\n",
    ")\n",
    "axes[2].set_xscale(\"log\")\n",
    "axes[2].set_yscale(\"log\")\n",
    "axes[2].set_xlabel(\"Number of Samples (Log Scale)\")\n",
    "axes[2].set_ylabel(\"Execution Time (s) (Log Scale)\")\n",
    "axes[2].set_title(\"ABOD Method\")\n",
    "axes[2].grid(True, which=\"both\", linestyle=\"--\", linewidth=0.5)\n",
    "\n",
    "fig.suptitle(\"Number of Samples vs. Execution Time per Outlier Detection Method\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see above, ABOD has more similar execution times as IF/LOF (<1s) when n<~200, but beyond that the time cost grows exponentially. IF and LOF distributions are irregular due to the extremely short processing times, and variation can be attributed to random noise; they generally follow an O(n) time complexity, while ABOD appears to follow O(n^2), which aligns with the pairwise computations required to calculate ABOD. \n",
    "\n",
    "To summarize, for the purpose of analyzing this dataset (or any large dataset n>~10000), IF is the clear choice to best balance processing time and model accuracy. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geo_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
